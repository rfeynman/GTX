"""Creates .hdf files from General Particle Tracer .gdf files

Author: Rory Speirs
Institution: School of Physics, The University of Melbourne.
Email: roryspeirs@gmail.com
Date: 13th June 2014
Version: 1.1
Licence: "THE BEER-WARE LICENSE" (Revision 42)

Fixes from Version 1.0:
Fixed gpthdf_to_slabhdf so it doesn't crash if you add or remove particles in GPT.

To use:
Type in terminal/command prompt: 
'python gdf_to_hdf.py [.gdf_directory] [.hdf directory] ['True' if hierical layout is also desired] ['HDFtoSLAB' if only conversion of pre-existing .hdf with hierical layout to slab layout is required]'

See function docstrings for specific information.
Main GDF conversion function is 'gdf_to_hdf'. Other functions are convenience functions.

This script was intentionally made similar to the 'loadgdf.m' MATLAB script written
by Merijn Rijnders, Peter Pasmans of Eindhoven University. If you understand one,
you will understand both.

Tested on python 2.7 on OSX 10.9.2 and Windows xp.

Your python version must have access to the modules below:
"""
from __future__ import division
from pylab import *
import h5py
import time
import struct
import os
import sys

###############################################################################
def gdf_to_hdf(gdf_file_directory, hdf_file_directory):
    """Copys a General Particle Tracer .gdf file into a more standard .hdf file.
    
    The resulting .hdf file has a highly hierical layout, similar to the layout 
    of the original .gdf. It can be opened by any standard hdf viewer or library.
    See http://www.hdfgroup.org/ for a GUI based viewer.
    
    The hierical structure is not necessarily the most convenient layout, so a
    companion function has also been written, 'gpthdf_to_slabhdf', which takes
    the .hdf file generated by 'gdf_to_hdf' and condenses all the data into one
    main multidimitional data array.
    """
    print 'Converting .gdf to .hdf file with hierical layout.'
    
    #Delete the .hdf file if it already exits to stop crashes from trying to overwrite datasets
    if os.path.exists(hdf_file_directory):
        os.remove(hdf_file_directory)
    
    hdf_f = h5py.File(hdf_file_directory, 'a')
    
    #Constants
    GDFNAMELEN = 16;                 #Length of the ascii-names
    GDFID  = 94325877;               #ID for GDF
    
    #Data types
    t_ascii  = int('0001', 16)       #ASCII character
    t_s32    = int('0002', 16)       #Signed long
    t_dbl    = int('0003', 16)       #Double
    t_undef  = int('0000', 16)       #Data type not defined
    t_null	 = int('0010', 16)       #No data
    t_u8	 = int('0020', 16)       #Unsigned char
    t_s8	 = int('0030', 16)       #Signed char
    t_u16	 = int('0040', 16)       #Unsigned short
    t_s16	 = int('0050', 16)       #Signed short
    t_u32	 = int('0060', 16)       #Unsigned long
    t_u64	 = int('0070', 16)       #Unsigned 64bit int
    t_s64	 = int('0080', 16)       #Signed 64bit int
    t_flt	 = int('0090', 16)       #Float

    #Block types
    t_dir    = 256      # Directory entry start
    t_edir   = 512      # Directory entry end
    t_sval   = 1024      # Single valued
    t_arr    = 2048      # Array
    
    with open(gdf_file_directory, 'rb') as f:   #Important to open in binary mode 'b' to work cross platform
    
        #Read the GDF main header
        
        gdf_id_check = struct.unpack('i', f.read(4))[0]
        if gdf_id_check != GDFID:
            raise RuntimeWarning('File directory is not a .gdf file')
        
        time_created = struct.unpack('i', f.read(4))[0]
        hdf_f.attrs['time_created'] = str(time_created) + ': ' + str(time.ctime(int(time_created)))
        
        #get creator name and use string part upto zero-character
        creator = list(f.read(GDFNAMELEN))
        creator = [struct.unpack('B', element)[0] for element in creator]
        creator_name = []
        for element in creator:
            if element is 0:
                break
            else:
                creator_name.append(chr(element))
        hdf_f.attrs['creator_name'] = ''.join(creator_name)

       #get destination and use string part upto zero-character
        dest = f.read(GDFNAMELEN)
        dest = [struct.unpack('B', element)[0] for element in dest]
        destination = []
        for element in dest:
            if element is 0:
                break
            else:
                destination.append(chr(element))
        hdf_f.attrs['destination'] = ''.join(destination)
        
        #get other metadata about the GDF file
        major = struct.unpack('B', f.read(1))[0]
        minor = struct.unpack('B', f.read(1))[0]
        hdf_f.attrs['gdf_version'] = str(major) + '.' + str(minor)
        
        major = struct.unpack('B', f.read(1))[0]
        minor = struct.unpack('B', f.read(1))[0]
        hdf_f.attrs['creator_version'] = str(major) + '.' + str(minor)
        
        major = struct.unpack('B', f.read(1))[0]
        minor = struct.unpack('B', f.read(1))[0]
        hdf_f.attrs['destination_version'] = str(major) + '.' + str(minor)

        f.seek(2, 1)   # skip to next block

        #Create first hdf group and sub groups for data to be put into
        #First group is called "datagrab" because it could be output at a particular time, or the projection at a particular position
        grab_group_number = 0
        grab_group = hdf_f.create_group('datagrab_' + str(grab_group_number))
        grab_group.attrs['grab_number'] = grab_group_number
        data_group = grab_group.create_group('data')
        param_group = grab_group.create_group('param')

        #Initialise values to print progress to terminal
        file_size = os.stat(gdf_file_directory)[6]
        start_time = time.time()
        last_running_time = 0

        #Read GDF data blocks
        lastarr = False
        while True:
            if f.read(1) == '':
                break
            f.seek(-1, 1)
            
            #Read GDF block header
            name = f.read(16)
            typee = struct.unpack('i', f.read(4))[0]
            size = struct.unpack('i', f.read(4))[0]

            #Get name
            name = name.split()[0]

            #Get block type
            dir  = int(typee & t_dir > 0)
            edir = int(typee & t_edir > 0)
            sval = int(typee & t_sval > 0)
            arr  = int(typee & t_arr > 0)

            #Get data type
            dattype = typee & 255
            
            #Check if array block is finished
            if lastarr and not arr:
                #We save the stuff as we go rather than storing it in local dictionaries and creating all the groups at the end. Here we make the groups for next time step, as this code only runs when all data current block has been extracted
                grab_group_number += 1
                grab_group = hdf_f.create_group('datagrab_' + str(grab_group_number))
                grab_group.attrs['grab_number'] = grab_group_number
                data_group = grab_group.create_group('data')
                param_group = grab_group.create_group('param')

            #Read single value
            if sval:
                if dattype == t_dbl:
                    value = struct.unpack('d', f.read(8))[0]
                    param_group.create_dataset(name, data=value)
                elif dattype == t_null:
                    pass
                elif dattype == t_ascii:
                    value = str(f.read(size))
                    value = value.strip(' \t\r\n\0')
                    try:
                        param_group.create_dataset(name, data=value)
                    except RuntimeError:
                        del param_group[name]
                        param_group.create_dataset(name, data=value)
                elif dattype == t_s32:
                    value = struct.unpack('i', f.read(4))[0]
                    param_group.create_dataset(name, data=value)
                else:
                    print 'unknown datatype of value!!!'
                    print 'name=', name
                    print 'type=', typee
                    print 'size=', size
                    value = f.read(size)
        
            #Read data array
            if arr:
                if dattype == t_dbl:
                    if (size % 8) != 0:
                        raise RuntimeWarning('Tried to save an array of doubles, but the array size is not consistant with that of doubles.')
                    value = fromfile(f, dtype=dtype('f8'), count=int(size/8))
                    data_group.create_dataset(name, data=value)
                else:
                    print 'unknown datatype of value!!!'
                    print 'name=', name
                    print 'type=', typee
                    print 'size=', size
                    value = f.read(size)
        
            #Print out progress approx. every 2 seconds
            running_time = int(round(time.time()-start_time))
            if (running_time % 2 is 0) and not (running_time is last_running_time):
                percent_done = f.tell()/file_size*100
                print 'Completed: %(completed).1f%(percent_sign)s' % {'completed':percent_done, 'percent_sign':'%'}
                last_running_time = running_time

            lastarr = arr;
    f.close()
    hdf_f.close()
    print 'Converting .gdf to .hdf file with hierical layout... Complete.'


###############################################################################
def gpthdf_to_slabhdf(gdf_hdf_file_directory, slab_hdf_file_directory):
    """Creates a .hdf with data in a single slab from a .hdf with GPT style layout.
    
    This actually produces two datasets: 'data' contains all the information about
    the particles, and 'position' or 'time' which contains the position or time
    that the data was saved in gpt.
    
    Note: It is possible to add and remove particles in GPT at different times.
    The slab will be a cube which is large enouth to fit the data assuming all 
    the particles exist at every datagrab. If particles don't actually exist at
    a datagrab (ie at a particular time or position), then the entries in the slab
    will be zeros for these particles. This makes it easy to see if a particle 
    exists at a certain time/position: just check for a zero in the ID column.
    
    See the 'slab_indexing' attribute of the 'data' dataset to see how the
    dataset is indexed.
    """
    print 'Creating .hdf file with slab layout.'
    
    #Delete the .hdf file if it already exits to stop crashes from trying to overwrite datasets
    if os.path.exists(slab_hdf_file_directory):
        os.remove(slab_hdf_file_directory)
    
    gpthdf_f = h5py.File(gdf_hdf_file_directory, 'a')       #Existing hdf file in hierical layout
    slabhdf_f = h5py.File(slab_hdf_file_directory, 'a')     #New hdf file with the slab layout
    
    #Transfer the original GDF file header to the attributes of the slab_hdf file
    for attribute_key in gpthdf_f.attrs.keys():
        slabhdf_f.attrs[attribute_key] = gpthdf_f.attrs[attribute_key]
    
    #Get information about the size of the required data slab
    datagrab_keys = gpthdf_f.keys()
    datagrab_n = len(datagrab_keys) - 1
    particle_n = int(gpthdf_f['datagrab_0/param/Nmp'][...])
    column_keys = gpthdf_f['datagrab_0/data/'].keys()
    
    #The last datagrab always contains some other parameters. Add these to the attributes of the slab_hdf file
    for dset_key in gpthdf_f['datagrab_' + str(datagrab_n) + '/param/'].keys():
        slabhdf_f.attrs[dset_key] = gpthdf_f['datagrab_' + str(datagrab_n) + '/param/' + dset_key][...]
    
    #Autodetect the data_grab_variable.
    if 'time' in gpthdf_f['datagrab_0/param/'].keys():
        data_grab_variable = 'time'
    elif 'position' in gpthdf_f['datagrab_0/param/'].keys():
        data_grab_variable = 'position'
    else:
        data_grab_variable = 'unknown_grab_variable'

    #Create empty datasets which will hold all the data, the "slabs".
    data_type_required = 'float64'
    dset_slab = slabhdf_f.require_dataset('data', (datagrab_n, particle_n, len(column_keys)), dtype=data_type_required)
    dset_grab_variable = slabhdf_f.require_dataset(data_grab_variable, (datagrab_n,), dtype=data_type_required)

    #Add attributes to the slab
    dset_slab.attrs['slab_indexing'] = '[' + data_grab_variable + '(0:' + str(datagrab_n) + '), particle_num(0:' + str(particle_n-1) + '), column(' + str(','.join(column_keys)) +') ]'
    for param_key in gpthdf_f['datagrab_0/param/'].keys():
        if str(param_key) == data_grab_variable:
            pass
        else:
            dset_slab.attrs[param_key] = gpthdf_f['datagrab_0/param/' + param_key]

    #Initialise values to print progress to terminal
    last_running_time = 0
    start_time = time.time()

    #Fill the data slab and time/position array
    for completion_count, grab_key in enumerate(datagrab_keys):
        grab_number = int(gpthdf_f[grab_key].attrs['grab_number'])
        if grab_number == datagrab_n:
            pass
        else:
            dset_grab_variable[grab_number] = gpthdf_f[grab_key + '/param/' + data_grab_variable][...]

            #Now fill the slab. Be Careful, sometimes not all particles actually exist.
            particle_IDs = gpthdf_f[grab_key + '/data/ID'][...]
            #Worth separating because it is faster to fill the whole column with a ':'
            if len(particle_IDs) == particle_n:
                for idx, column_key in enumerate(column_keys):
                    dset_column = gpthdf_f[grab_key + '/data/' + column_key]
                    dset_slab[grab_number, :, idx] = dset_column[:]
            else:
                for idx, column_key in enumerate(column_keys):
                    dset_column = gpthdf_f[grab_key + '/data/' + column_key]
                    dset_slab[grab_number, particle_IDs-1, idx] = dset_column[:]

        #Print out progress every 2 seconds
        running_time = int(round(time.time()-start_time))
        if (running_time % 2 is 0) and not (running_time is last_running_time):
            percent_done = completion_count/datagrab_n*100
            print 'Completed: %(completed).1f%(percent_sign)s' % {'completed':percent_done, 'percent_sign':'%'}
            last_running_time = running_time

    gpthdf_f.close()
    slabhdf_f.close()
    print 'Creating .hdf file with slab layout... Complete.'


###############################################################################
def terminal_call(terminal_args):
    """Allows arguments to be conveniently passed from the terminal/command line so there is no need to edit this python script.
    
    To use, type: 'python gdf_to_hdf.py .gdf_directory [.hdf directory] ['True' if hierical layout is also desired] ['HDFtoSLAB' if only conversion of pre-existing .hdf with hierical layout to slab layout is required]'
    # Note: order of arguments in '[]' is irrelivant and hierical file will be 
    appended with '_hierical.hdf', or if conversion from hierical to slab, then
    slab file will be appended with '_slab.hdf'. Conversions can only be done
    from .gdf to hierical layout .hdf to slab layout .hdf, and not the other
    way around.
    
    """
    hierical_suffix = '_hierical'
    slab_suffix = '_slab'
    
    HDFtoSLAB = False
    gdf_arg = False
    hdf_arg = False
    keep_hierical = False

    #
    for arg in terminal_args:
        if arg[-4:] == '.gdf':
            gdf_file_directory = arg
            gdf_arg = True
        elif arg[-4:] == '.hdf':
            hdf_file_directory = arg
            hdf_arg = True
        elif arg in ['True', 'true', 'TRUE', 't', 'T', 'Yes', 'yes', 'YES', 'y', 'Y']:
            keep_hierical = True
        elif arg in ['HDFtoSLAB']:
            HDFtoSLAB = True

    # If this option, only action is convert a pre-existing .hdf that is in hierical layout
    if HDFtoSLAB:
        if hdf_arg:
            if os.path.exists(hdf_file_directory):
                hdf_slab_file_directory = hdf_file_directory[:-4] + slab_suffix + hdf_file_directory[-4:]
                gpthdf_to_slabhdf(hdf_file_directory, hdf_slab_file_directory)
            else:
                print 'The .hdf file does not exist to convert to slab layout.'
        else:
            print 'The .hdf file was not specified.'

    # If this option, convert .gdf to .hdf slab layout, and optionally keep the .hdf with hierical layout that is generated along the way.
    elif gdf_arg:
        if os.path.exists(gdf_file_directory):
            if not hdf_arg:
                hdf_slab_file_directory = gdf_file_directory[:-4] + '.hdf'
                hdf_file_directory = gdf_file_directory[:-4] + hierical_suffix + '.hdf'
                print 'Destination .hdf directory not specified. Defaulting to ' + hdf_slab_file_directory
            else:
                hdf_slab_file_directory = hdf_file_directory
                hdf_file_directory = hdf_slab_file_directory[:-4] + hierical_suffix + '.hdf'
            gdf_to_hdf(gdf_file_directory, hdf_file_directory)
            gpthdf_to_slabhdf(hdf_file_directory, hdf_slab_file_directory)
            if not keep_hierical:
                print 'Removing .hdf with hierical layout.'
                os.remove(hdf_file_directory)
        else:
            print 'The .gdf file does not exist to convert to .hdf'
    else:
        print '.gdf file not specified to convert to .hdf, or \'HDFtoSLAB\' option not selected to convert .hdf to slab layout'


###############################################################################
def script_call():
    """Allows conversion by editing of filenames in this script. Consider passing arguments directly from the terminal for convenience.
    """
    gdf_file_directory = 'gptfile.gdf'
    hdf_file_directory = gdf_file_directory[:-4] + '.hdf'
    slab_hdf_file_directory = gdf_file_directory[:-4] + '_slab.hdf'
    
    #Convert GDF to HDF with heirical layout
    gdf_to_hdf(gdf_file_directory, hdf_file_directory)

    #Create a new HDF file with the easier to use slab layout
    gpthdf_to_slabhdf(hdf_file_directory, slab_hdf_file_directory)

###############################################################################
#Make program run now...
if __name__ == "__main__":
    # Terminal use: 'python gdf_to_hdf.py [.dgf dir] [.hdf dir] ['True' if slab desired] ['HDFtoSLAB' if only conversion of pre-existing .hdf is required]'
    # Note: order of arguments in '[]' is irrelivant
    
    terminal_args = sys.argv
    if len(terminal_args) is 1:
        script_call()               #Run 'script_call' function if no terminal arguments passed
    else:
        terminal_call(terminal_args)#Run 'terminal_call' function arguments are passed
